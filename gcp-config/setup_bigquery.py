#!/usr/bin/env python3
"""
BigQuery Setup Script

This script creates the necessary BigQuery datasets and tables for the IoT anomaly detection system.
"""

import argparse
from google.cloud import bigquery
from google.api_core import exceptions

def create_dataset(client: bigquery.Client, project_id: str, dataset_id: str) -> None:
    """Create a BigQuery dataset if it doesn't exist."""
    dataset_ref = client.dataset(dataset_id, project=project_id)
    
    try:
        dataset = bigquery.Dataset(dataset_ref)
        dataset.location = "US"  # Set the location for the dataset
        dataset.description = "Dataset for IoT anomaly detection system"
        
        dataset = client.create_dataset(dataset, timeout=30)
        print(f"Created dataset {dataset.dataset_id}")
    except exceptions.Conflict:
        print(f"Dataset {dataset_id} already exists")

def create_sensor_data_table(client: bigquery.Client, dataset_ref: bigquery.DatasetReference) -> None:
    """Create the sensor data table."""
    table_id = "sensor_data"
    table_ref = dataset_ref.table(table_id)
    
    schema = [
        bigquery.SchemaField("device_id", "STRING", mode="REQUIRED"),
        bigquery.SchemaField("device_type", "STRING", mode="REQUIRED"),
        bigquery.SchemaField("location", "STRING", mode="REQUIRED"),
        bigquery.SchemaField("timestamp", "TIMESTAMP", mode="REQUIRED"),
        bigquery.SchemaField("processed_at", "TIMESTAMP", mode="NULLABLE"),
        bigquery.SchemaField("temperature", "FLOAT", mode="REQUIRED"),
        bigquery.SchemaField("humidity", "FLOAT", mode="REQUIRED"),
        bigquery.SchemaField("pressure", "FLOAT", mode="REQUIRED"),
        bigquery.SchemaField("vibration_level", "FLOAT", mode="NULLABLE"),
        bigquery.SchemaField("power_consumption", "FLOAT", mode="NULLABLE"),
        bigquery.SchemaField("is_anomaly", "BOOLEAN", mode="REQUIRED"),
        bigquery.SchemaField("anomaly_type", "STRING", mode="NULLABLE"),
        bigquery.SchemaField("anomaly_score", "FLOAT", mode="NULLABLE"),
        bigquery.SchemaField("rule_based_anomaly", "BOOLEAN", mode="NULLABLE"),
        bigquery.SchemaField("ml_anomaly_score", "FLOAT", mode="NULLABLE"),
        bigquery.SchemaField("ml_anomaly_prediction", "BOOLEAN", mode="NULLABLE"),
    ]
    
    try:
        table = bigquery.Table(table_ref, schema=schema)
        table.description = "IoT sensor data with anomaly detection results"
        
        # Set up partitioning by timestamp for better performance
        table.time_partitioning = bigquery.TimePartitioning(
            type_=bigquery.TimePartitioningType.DAY,
            field="timestamp"
        )
        
        table = client.create_table(table, timeout=30)
        print(f"Created table {table.table_id}")
    except exceptions.Conflict:
        print(f"Table {table_id} already exists")

def create_anomaly_alerts_table(client: bigquery.Client, dataset_ref: bigquery.DatasetReference) -> None:
    """Create the anomaly alerts table."""
    table_id = "anomaly_alerts"
    table_ref = dataset_ref.table(table_id)
    
    schema = [
        bigquery.SchemaField("alert_id", "STRING", mode="REQUIRED"),
        bigquery.SchemaField("device_id", "STRING", mode="REQUIRED"),
        bigquery.SchemaField("alert_timestamp", "TIMESTAMP", mode="REQUIRED"),
        bigquery.SchemaField("anomaly_type", "STRING", mode="REQUIRED"),
        bigquery.SchemaField("severity", "STRING", mode="REQUIRED"),
        bigquery.SchemaField("anomaly_score", "FLOAT", mode="REQUIRED"),
        bigquery.SchemaField("sensor_values", "JSON", mode="NULLABLE"),
        bigquery.SchemaField("alert_status", "STRING", mode="REQUIRED"),
        bigquery.SchemaField("acknowledged_by", "STRING", mode="NULLABLE"),
        bigquery.SchemaField("acknowledged_at", "TIMESTAMP", mode="NULLABLE"),
        bigquery.SchemaField("resolved_at", "TIMESTAMP", mode="NULLABLE"),
    ]
    
    try:
        table = bigquery.Table(table_ref, schema=schema)
        table.description = "Anomaly alerts generated by the detection system"
        
        # Set up partitioning by alert timestamp
        table.time_partitioning = bigquery.TimePartitioning(
            type_=bigquery.TimePartitioningType.DAY,
            field="alert_timestamp"
        )
        
        table = client.create_table(table, timeout=30)
        print(f"Created table {table.table_id}")
    except exceptions.Conflict:
        print(f"Table {table_id} already exists")

def create_model_performance_table(client: bigquery.Client, dataset_ref: bigquery.DatasetReference) -> None:
    """Create the model performance tracking table."""
    table_id = "model_performance"
    table_ref = dataset_ref.table(table_id)
    
    schema = [
        bigquery.SchemaField("model_name", "STRING", mode="REQUIRED"),
        bigquery.SchemaField("model_version", "STRING", mode="REQUIRED"),
        bigquery.SchemaField("evaluation_timestamp", "TIMESTAMP", mode="REQUIRED"),
        bigquery.SchemaField("precision", "FLOAT", mode="NULLABLE"),
        bigquery.SchemaField("recall", "FLOAT", mode="NULLABLE"),
        bigquery.SchemaField("f1_score", "FLOAT", mode="NULLABLE"),
        bigquery.SchemaField("accuracy", "FLOAT", mode="NULLABLE"),
        bigquery.SchemaField("auc_roc", "FLOAT", mode="NULLABLE"),
        bigquery.SchemaField("false_positive_rate", "FLOAT", mode="NULLABLE"),
        bigquery.SchemaField("false_negative_rate", "FLOAT", mode="NULLABLE"),
        bigquery.SchemaField("training_data_size", "INTEGER", mode="NULLABLE"),
        bigquery.SchemaField("evaluation_data_size", "INTEGER", mode="NULLABLE"),
    ]
    
    try:
        table = bigquery.Table(table_ref, schema=schema)
        table.description = "Model performance metrics for anomaly detection models"
        
        table = client.create_table(table, timeout=30)
        print(f"Created table {table.table_id}")
    except exceptions.Conflict:
        print(f"Table {table_id} already exists")

def main():
    """Main function to set up BigQuery resources."""
    parser = argparse.ArgumentParser(description='Setup BigQuery datasets and tables')
    parser.add_argument('--project-id', required=True, help='Google Cloud Project ID')
    parser.add_argument('--dataset-id', default='iot_anomaly_detection', help='BigQuery dataset ID')
    
    args = parser.parse_args()
    
    # Initialize BigQuery client
    client = bigquery.Client(project=args.project_id)
    
    # Create dataset
    create_dataset(client, args.project_id, args.dataset_id)
    
    # Get dataset reference
    dataset_ref = client.dataset(args.dataset_id, project=args.project_id)
    
    # Create tables
    create_sensor_data_table(client, dataset_ref)
    create_anomaly_alerts_table(client, dataset_ref)
    create_model_performance_table(client, dataset_ref)
    
    print(f"\nBigQuery setup completed!")
    print(f"Dataset: {args.dataset_id}")
    print("Tables created:")
    print("  - sensor_data")
    print("  - anomaly_alerts")
    print("  - model_performance")

if __name__ == "__main__":
    main()

